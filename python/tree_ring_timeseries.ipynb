{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read tree ring chronology data files and write them to JSON\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timeseries():\n",
    "    root_dir = \"C:/Users/Jacob/Projects/data_itrdb\"\n",
    "    data_file = \"C:/Users/Jacob/Projects/data_itrdb/itrdb_chronology_data.txt\"\n",
    "    crn_json = {}\n",
    "    rwl_json = {}\n",
    "    corr_json = {}\n",
    "    \n",
    "    errorsfile = open(os.path.join(root_dir, \"timeseries_errors.txt\"), \"w\")\n",
    "    in_file = open(data_file)\n",
    "    line = in_file.readline()\n",
    "    line = in_file.readline()\n",
    "    count = 0\n",
    "    while line != \"\":\n",
    "        line = line.strip().split(\",\")\n",
    "        site_id = line[0]\n",
    "        data_urls = []\n",
    "        for n in [1,16,31,46,61,76]:\n",
    "            if n < len(line):\n",
    "                data_urls.append(line[n])\n",
    "        for f in data_urls:\n",
    "            basename = os.path.basename(f)\n",
    "            if f == \" \" or f == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    with urllib.request.urlopen(f) as url:\n",
    "                        html_doc = url.read()\n",
    "                        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                    try:\n",
    "                        if f[-4:] == \".crn\":\n",
    "                            read_crn(soup, crn_json, basename)\n",
    "                        elif f[-4:] == \".txt\":\n",
    "                            read_correlation_stats(soup, corr_json, basename)\n",
    "                        elif f[-4:] == \".rwl\":\n",
    "                            read_rwl(soup, rwl_json, basename)\n",
    "                    except:\n",
    "                        errorsfile.write(str(f) + \"\\n\")\n",
    "                except:\n",
    "                    print(\"cannot open: \" + str(f))\n",
    "        count+=1\n",
    "        print(count)\n",
    "        line = in_file.readline()\n",
    "    # dump the jsons. chyeah boiiiyiyyyyyy!!\n",
    "    errorsfile.close()\n",
    "    with open('./itrdb_crn_data.json', 'w') as outfile:\n",
    "        json.dump(crn_json, outfile)\n",
    "    with open('./itrdb_rwl_data.json', 'w') as outfile:\n",
    "        json.dump(rwl_json, outfile)\n",
    "    with open('./itrdb_corr_data.json', 'w') as outfile:\n",
    "        json.dump(corr_json, outfile)\n",
    "    \n",
    "\n",
    "    \n",
    "def read_crn(soup, in_json, basename):\n",
    "    \"\"\"\n",
    "    basename here is the basename of the url data file that is contained in the paleodata file\n",
    "    {\n",
    "     basenameA: [[values], [samples], [start_year, end_year]],\n",
    "     basenameB: [[values], [samples], [start_year, end_year]]\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    # processed according to: ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/treeinfo.txt\n",
    "    # there are three common flavors, sitecodeR.crn, sitecodeA.crn, and sitecode.crn\n",
    "    if basename not in in_json:\n",
    "        in_json[basename] = [[basename],[\"sample number\"],[]]\n",
    "    conts = soup.contents\n",
    "    conts = conts[0].split(\"\\r\\n\")\n",
    "    # start at the third line, because most .crn have three lines of header?\n",
    "    record = 0\n",
    "    missing_value = 9990\n",
    "    start = False\n",
    "    while record < len(conts) and conts[record] != \"\":\n",
    "        if record == 0:\n",
    "            siteid = conts[record][0:6].strip()\n",
    "        elif record == 1:\n",
    "            start_year = conts[record][67:71]\n",
    "            end_year = conts[record][72:76]\n",
    "            in_json[basename][2] = [int(start_year), int(end_year)]\n",
    "            countyear = int(start_year)\n",
    "        elif record == 2:\n",
    "            pass\n",
    "        else:\n",
    "            # process the file\n",
    "            idx = 10\n",
    "            while idx < 80:\n",
    "                val = conts[record][idx:idx+4].strip()\n",
    "                if val != \"9990\":\n",
    "                    if start == False: # do this funny business to catch when to begin counting years.\n",
    "                        start = True\n",
    "                    countyear+=1\n",
    "                    samp_num = conts[record][idx+4:idx+7].strip()\n",
    "                    in_json[basename][0].append(float(val))\n",
    "                    if samp_num != \"\":\n",
    "                        in_json[basename][1].append(int(samp_num))\n",
    "                    else:\n",
    "                        in_json[basename][1].append(None)\n",
    "                else:\n",
    "                    if start: # there might be gaps in the chronology, so we still need to count the years in gaps\n",
    "                        countyear+=1\n",
    "                idx+=7\n",
    "                if countyear >= int(end_year):\n",
    "                    idx=80\n",
    "        record+=1\n",
    "    \n",
    "def read_correlation_stats(soup, in_json, basename):\n",
    "    \"\"\"\n",
    "    basenameA: {seriesIntercorrelation: val,\n",
    "                avgmeansens: val,\n",
    "                }\n",
    "    \"\"\"\n",
    "    if basename not in in_json:\n",
    "        in_json[basename] = {}\n",
    "    conts = soup.contents\n",
    "    conts = conts[0].split(\"\\n\")\n",
    "    fields = {\"Series intercorrelation\":\"serICorr\",\n",
    "              \"Avg mean sensitivity\":\"avgMeanSens\",\n",
    "              \"Avg standard deviation\":\"avgStd\",\n",
    "              \"Avg autocorrelation\":\"avgAutoCorr\",\n",
    "              \"Number dated series\":\"nSeries\",\n",
    "              \"Number problem segments\":\"nProbSeg\",\n",
    "              \"Pct problem segments\":\"pctProbSeg\",\n",
    "              \"Segment length tested\":\"segLenTest\"}\n",
    "    line = 0\n",
    "    done = False\n",
    "    while line < len(conts) and not done:\n",
    "        dataline = conts[line].split(\":\")\n",
    "        if dataline[0].strip() in fields:\n",
    "            in_json[basename][fields[dataline[0].strip()]] = dataline[1].strip()\n",
    "        if len(in_json[basename]) == 8:\n",
    "            done = True\n",
    "        line+=1\n",
    "    \n",
    "def read_rwl(soup, in_json, basename):\n",
    "    \"\"\"\n",
    "    {\n",
    "     basenameA: {treecore1: [[values], [start_year, end_year]], treecore2: [[values], [start_year, end_year]]},\n",
    "     basenameB: {treecore2: [[values], [start_year, end_year]], treecore2: [[values], [start_year, end_year]]}\n",
    "    }\n",
    "    \"\"\"\n",
    "    # processed according to: ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/treeinfo.txt\n",
    "    # there are three common flavors, sitecodeR.crn, sitecodeA.crn, and sitecode.crn\n",
    "    if basename not in in_json:\n",
    "        in_json[basename] = {}\n",
    "    conts = soup.contents\n",
    "    conts = conts[0].split(\"\\r\\n\")\n",
    "    record = 0\n",
    "    while record < len(conts) and conts[record] != \"\":\n",
    "        if record == 0:\n",
    "            siteid = conts[record][0:6].strip()\n",
    "        elif record == 1:\n",
    "            pass\n",
    "            \"\"\"start_year = conts[record][67:71]\n",
    "            end_year = conts[record][72:76]\n",
    "            in_json[basename][2] = [int(start_year), int(end_year)]\"\"\"\n",
    "        elif record == 2:\n",
    "            pass\n",
    "        else:\n",
    "            tree_core_id = conts[record][:6]\n",
    "            if tree_core_id not in in_json[basename]:\n",
    "                year = int(conts[record][8:12].strip())\n",
    "                in_json[basename][tree_core_id] = [[tree_core_id],[\"year\"]]\n",
    "                count = 0\n",
    "            # process the file\n",
    "            idx = 12\n",
    "            while idx < 73:\n",
    "                val = conts[record][idx:idx+6].strip()\n",
    "                if val != \"\" and val != '-9999' and val != '999':\n",
    "                    in_json[basename][tree_core_id][0].append(float(val))\n",
    "                    in_json[basename][tree_core_id][1].append(year+count)\n",
    "                    count+=1\n",
    "                elif val == '-9999' or val == '999':\n",
    "                    in_json[basename][tree_core_id][0].append(None)\n",
    "                    in_json[basename][tree_core_id][1].append(year+count)\n",
    "                    count+=1\n",
    "                idx+=6\n",
    "        record+=1\n",
    "        \n",
    "process_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.grid()\n",
    "plt.plot(out['wv003.rwl']['033011'][1][1:], out['wv003.rwl']['033011'][0][1:])\n",
    "plt.plot(out['wv003.rwl']['010021'][1][1:], out['wv003.rwl']['010021'][0][1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
